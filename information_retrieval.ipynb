{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddea025-fb05-4d45-8fde-5060a8b6b388",
   "metadata": {},
   "source": [
    "# Information retrieval\n",
    "\n",
    "Information Retrieval (IR) is a Natural Language Processing (NLP) task, in which the objective is to retrieve relevant information within a corpus of text, based on a query. For this purpose, text encoder models are trained to represent texts using `embeddings` (i.e., vectors of numbers), aiming to correctly represent the meaning of the text. Finally, the model can also encode the query, and hopefully, the query embedding will be closer to text embeddings that are meaningful to answer the query. \n",
    "\n",
    "This notebooks show how to perform Information Retrieval to retrieve relevant spans of texts within a set of documents. This will be divided into 3 different sections : \n",
    "1. **Document processing** : the 1st step is to extract spans of texts from all the desired documents.\n",
    "2. **Text encoding** : the 2nd step is to encode all the spans of texts using an appropriate embedding model. For this demonstration, we will use the [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) model,\n",
    "3. **Search query** : finally, the last step is to encode the query by using the same model, and retrieve the spans with the lowest distance (or higher similarity) with the query embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8ec35-3787-4ecc-9cfd-ec1eb8436af5",
   "metadata": {},
   "source": [
    "## 1. Document processing\n",
    "\n",
    "The document processing step aims to extract texts from documents. The `parse_document` method accepts filenames, directories and filename format (as below), and returns a list of paragraphs. This is more convenient compared to full text extraction for futher processing, like encoding the texts ;) The method currently accepts `.txt`, `.md`, `.pdf` and `.docs` file formats, and more will be added in the future !\n",
    "\n",
    "For this demonstration, I will use the `README` files from all my github repositories. This will also be easier to evaluate the relevance of the retrieved documents ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c852972-e22b-43bf-ad3f-34385d35334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# texts : 323\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>section</th>\n",
       "      <th>section_titles</th>\n",
       "      <th>filename</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># :yum: Data processing utilities</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check the CHANGELOG file to have a global over...</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>## Project structure</td>\n",
       "      <td>text</td>\n",
       "      <td>1.1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check the provided notebooks to have an overvi...</td>\n",
       "      <td>text</td>\n",
       "      <td>1.1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>├── example_data        : data used for the de...</td>\n",
       "      <td>code</td>\n",
       "      <td>1.1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "      <td>bash</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  type section  \\\n",
       "0                  # :yum: Data processing utilities  text       1   \n",
       "1  Check the CHANGELOG file to have a global over...  text       1   \n",
       "2                               ## Project structure  text     1.1   \n",
       "3  Check the provided notebooks to have an overvi...  text     1.1   \n",
       "4  ├── example_data        : data used for the de...  code     1.1   \n",
       "\n",
       "                                      section_titles  \\\n",
       "0  [:yum: Data processing utilities, Project stru...   \n",
       "1  [:yum: Data processing utilities, Project stru...   \n",
       "2  [:yum: Data processing utilities, Project stru...   \n",
       "3  [:yum: Data processing utilities, Project stru...   \n",
       "4  [:yum: Data processing utilities, Project stru...   \n",
       "\n",
       "                       filename language  \n",
       "0  ../data_processing/README.md      NaN  \n",
       "1  ../data_processing/README.md      NaN  \n",
       "2  ../data_processing/README.md      NaN  \n",
       "3  ../data_processing/README.md      NaN  \n",
       "4  ../data_processing/README.md     bash  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils.text import parse_document\n",
    "\n",
    "documents = parse_document('../**/README.md')\n",
    "documents = pd.DataFrame(documents)\n",
    "print('# texts : {}'.format(len(documents)))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5935181-3802-4327-accc-17c0e7ef0417",
   "metadata": {},
   "source": [
    "## 2. Text encoding\n",
    "\n",
    "Now that we have all the texts extracted with additional information (like section title / filename), we can encode them using embeddings ! For this purpose, let's initialize a `TextEncoder` model with the [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) model. Then, we can use this model to encode the texts by using the `embed` method. \n",
    "\n",
    "`embed` is a batched function, meaning that you can provide the `batch_size` argument to control the number of texts to encode in parallel. An important aspect to consider is that texts are padded when passing in parallel, in order to form a rectangular matrix (i.e., the smaller texts have zero-values at the end so that all texts within a batch have the same length). The function has been optimized by sorting the texts by length, in order to minimize padding. However, it remains interesting to correctly tune the `batch_size`, as it has a large impact on performances ! My recommandation would be to use a small value, around 8. \n",
    "\n",
    "The model is compiled using `XLA` by default, which explains why some calls are slower than subsequent ones, due to retracing. \n",
    "\n",
    "At the 1st call, the official `transformers` model will be downloaded and converted to my `keras` implementation of the `XLMRoberta` architecture. For this purpose, you will need the `torch` library to be installed. Once done, the model will be saved in regular keras format under the `pretrained_models/{name}` folder for subsequent loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b314a0b8-7b17-4a88-99b2-ba44ae7a8e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 08:57:32.681846: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-25 08:57:32.688544: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748156252.696001    3787 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748156252.698133    3787 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748156252.703711    3787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748156252.703718    3787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748156252.703719    3787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748156252.703720    3787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-25 08:57:32.705748: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1748156254.407256    3787 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22438 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090 Ti, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from `pretrained_models/bge-m3/saving/ckpt-0000.weights.h5`\n",
      "TextEncoder `bge-m3` initialized successfully !\n",
      "\n",
      "========== bge-m3 ==========\n",
      "Model :\n",
      "- Inputs \t: unknown\n",
      "- Outputs \t: unknown\n",
      "- Number of layers \t: 26\n",
      "- Number of parameters \t: 567.756 Millions\n",
      "- Model not compiled yet\n",
      "\n",
      "Transfer-learning from : BAAI/bge-m3\n",
      "Already trained on 0 epochs (0 steps)\n",
      "\n",
      "- Embedding dim   : 1024\n",
      "- Distance metric : cosine\n",
      "- Language : multi\n",
      "- Vocabulary (size = 250002) : ['<s>', '<pad>', '</s>', '<unk>', ',', '.', '▁', 's', '▁de', '-', '▁a', 'a', ':', 'e', 'i', '▁(', ')', '▁i', 't', 'n', '▁-', '▁la', '▁en', '▁in', '▁na', ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.encoder.text_encoder import TextEncoder\n",
    "\n",
    "model = TextEncoder(pretrained = 'BAAI/bge-m3', name = 'bge-m3')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5e2c91-26c8-43da-9f8c-4d3bc3b288b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VectorDatabase path=documents.db key=('filename', 'text') length=133>\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "vectors = model.predict(\n",
    "    documents  = '../**/README.md',\n",
    "    batch_size = 8,\n",
    "\n",
    "    save         = False,\n",
    "    chunk_size   = 256,\n",
    "    group_by    = ('filename', 'section_titles'),\n",
    "    primary_key = ('filename', 'text'),\n",
    "    \n",
    "    tqdm = tqdm,\n",
    ")\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc93ccc-1eee-4e36-a9a9-6be7a06eb2aa",
   "metadata": {},
   "source": [
    "## 3. Search query\n",
    "\n",
    "The final step is to encode the query, then compute the `cosine similarity` (or any other distance/similarity metric) between the embedded query and all embedded texts, and take the top-k with the best score ! All these steps are performed internally by the `search` method of the `DenseVectors` class ;)\n",
    "\n",
    "We can observe that the best results are correctly related to `embeddings`, and even more, the best retrieved passage correctly defines the notion of embeddings !\n",
    "\n",
    "It is worth mentioning that the model will retrieve passages no matter if they are relevant or not, as it simply provides a score for each passage. Therefore, if the query does not have any relevant span in the provided text, it will return irrelevant spans. Nonetheless, as it can be observed in the 2nd example, scores for such irreevant passages (in the 2nd example) is lower than relevant ones (in the 1st example) ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbb7c8-ef9a-40bb-9969-fd8dc1e25761",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10158c9-7469-4636-a1b1-66c6be814a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"section_titles\": [\n",
      "            \":yum: Encoder networks\",\n",
      "            \"Project structure\"\n",
      "        ],\n",
      "        \"language\": \"bash\",\n",
      "        \"filename\": \"../encoders/README.md\",\n",
      "        \"text\": \"text encoder that uses pretrained embedding models\\n\\u251c\\u2500\\u2500 pretrained_models\\n\\u251c\\u2500\\u2500 unitests\\n\\u251c\\u2500\\u2500 utils\\n\\u251c\\u2500\\u2500 speaker_verification.ipynb\\n\\u2514\\u2500\\u2500 information_retrieval.ipynb Check the main project for more information about the unextended modules / structure / main classes.\\n\\n **Important Note** : this project is the keras 3 extension of the siamese network project. All features are not available yet. Once the convertion will be completely finished, the siamese networks project will be removed in favor of this one.\",\n",
      "        \"score\": 0.48156794905662537\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"text\",\n",
      "        \"section_titles\": [\n",
      "            \":yum: Text To Speech (TTS)\",\n",
      "            \"Multi-speaker Text-To-Speech\",\n",
      "            \"Automatic voice cloning with the `SV2TTS` architecture\",\n",
      "            \"The basic intuition\"\n",
      "        ],\n",
      "        \"filename\": \"../text_to_speech/README.md\",\n",
      "        \"text\": \"2. This pre-trained `Speaker Encoder (SE)` is then used to encode the voice of the speaker to clone.\\n 3. The produced embedding is then concatenated with the output of the `Tacotron-2` encoder part, such that the `Decoder` has access to both the encoded text and the speaker embedding.\\n\\n The objective is that the `Decoder` will learn to use the `speaker embedding` to copy its prosody/intonation/etc. to read the text with the voice of this speaker.\",\n",
      "        \"score\": 0.47130608558654785\n",
      "    },\n",
      "    {\n",
      "        \"type\": \"text\",\n",
      "        \"section_titles\": [\n",
      "            \":yum: Text To Speech (TTS)\",\n",
      "            \"Multi-speaker Text-To-Speech\",\n",
      "            \"Automatic voice cloning with the `SV2TTS` architecture\",\n",
      "            \"The basic intuition\"\n",
      "        ],\n",
      "        \"filename\": \"../text_to_speech/README.md\",\n",
      "        \"text\": \"#### The basic intuition\\n\\n The `Speaker Encoder-based Text-To-Speech` is inspired from the \\\"From Speaker Verification To Text-To-Speech (SV2TTS)\\\" paper. The authors have proposed an extension of the `Tacotron-2` architecture to include information about the speaker's voice.\\n\\n Here is a short overview of the proposed procedure:\\n 1. Train a model to identify speakers based on short audio samples: the `speaker verification` model. This model takes as input an audio sample (5-10 sec) from a speaker and encodes it into a *d*-dimensional vector, named the `embedding`. This embedding aims to capture relevant information about the speaker's voice (e.g., `frequencies`, `rhythm`, `pitch`, etc.). \\n 2. This pre-trained `Speaker Encoder (SE)` is then used to encode the voice of the speaker to clone.\\n\",\n",
      "        \"score\": 0.4611942172050476\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from utils import to_json\n",
    "\n",
    "query = 'What is an embedding ?'\n",
    "\n",
    "res = model.retrieve(query, vectors, k = 3)[0]\n",
    "print(json.dumps(to_json(res), indent = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2760e1-94f9-4150-947c-b47a7461d363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from file `../encoders/README.md` - section [':yum: Encoder networks', 'Project structure']\n",
      "Score : 0.482\n",
      "text encoder that uses pretrained embedding models\n",
      "├── pretrained_models\n",
      "├── unitests\n",
      "├── utils\n",
      "├── speaker_verification.ipynb\n",
      "└── information_retrieval.ipynb Check the main project for more information about the unextended modules / structure / main classes.\n",
      "\n",
      " **Important Note** : this project is the keras 3 extension of the siamese network project. All features are not available yet. Once the convertion will be completely finished, the siamese networks project will be removed in favor of this one.\n",
      "\n",
      "Text from file `../text_to_speech/README.md` - section [':yum: Text To Speech (TTS)', 'Multi-speaker Text-To-Speech', 'Automatic voice cloning with the `SV2TTS` architecture', 'The basic intuition']\n",
      "Score : 0.471\n",
      "2. This pre-trained `Speaker Encoder (SE)` is then used to encode the voice of the speaker to clone.\n",
      " 3. The produced embedding is then concatenated with the output of the `Tacotron-2` encoder part, such that the `Decoder` has access to both the encoded text and the speaker embedding.\n",
      "\n",
      " The objective is that the `Decoder` will learn to use the `speaker embedding` to copy its prosody/intonation/etc. to read the text with the voice of this speaker.\n",
      "\n",
      "Text from file `../text_to_speech/README.md` - section [':yum: Text To Speech (TTS)', 'Multi-speaker Text-To-Speech', 'Automatic voice cloning with the `SV2TTS` architecture', 'The basic intuition']\n",
      "Score : 0.461\n",
      "#### The basic intuition\n",
      "\n",
      " The `Speaker Encoder-based Text-To-Speech` is inspired from the \"From Speaker Verification To Text-To-Speech (SV2TTS)\" paper. The authors have proposed an extension of the `Tacotron-2` architecture to include information about the speaker's voice.\n",
      "\n",
      " Here is a short overview of the proposed procedure:\n",
      " 1. Train a model to identify speakers based on short audio samples: the `speaker verification` model. This model takes as input an audio sample (5-10 sec) from a speaker and encodes it into a *d*-dimensional vector, named the `embedding`. This embedding aims to capture relevant information about the speaker's voice (e.g., `frequencies`, `rhythm`, `pitch`, etc.). \n",
      " 2. This pre-trained `Speaker Encoder (SE)` is then used to encode the voice of the speaker to clone.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paragraph in res:\n",
    "    print('Text from file `{filename}` - section {section_titles}\\nScore : {score:.3f}\\n{text}\\n'.format(** paragraph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbde3d0-0832-4a46-a6cd-740f50d0e116",
   "metadata": {},
   "source": [
    "### Example 2 : irrelevant query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bc3b922-3386-4a6b-bf79-d22da07b6faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from file `../detection/README.md`\n",
      "Score : 0.403\n",
      "1]` |\n",
      "| Applications | General detection + classification | Medical image detection / object extraction |\n",
      "| Model architecture | Full CNN 2D downsampling to `(grid_h, grid_w)` | Full CNN with downsampling and upsampling |\n",
      "| Post processing | Decode output to get position of boxes | Thresholding pixel confidence |\n",
      "| Model mechanism | Split image into grid and detect boxes in each grid cell | Downsample the image and upsample it to give probability of object for each pixel |\n",
      "| Support multi-label classification | Yes, by design | Yes, but not its main application | \\* This is the classical output shape of `YOLO` models. The last dimension is `[x, y, w, h, confidence, * class_score]`\n",
      "\n",
      " More advanced strategies also exist, differing from the standard methodologies described above. This aims to be a simple introduction to object detection and segmentation.\n",
      "\n",
      "Text from file `../yui-mhcp/README.md`\n",
      "Score : 0.400\n",
      "<h2 align=\"center\">\n",
      "<p> :yum: Yui-mhcp :yum: </p>\n",
      "</h2>\n",
      "\n",
      "<h2 align=\"center\">\n",
      "<p> A centralization of Deep Learning projects </p>\n",
      "</h2>\n",
      "\n",
      "Welcome to my GitHub profile! The objective of my projects is to make Deep Learning more accessible, and provide real-world examples to enhance education and research in the field! :yum:\n",
      "\n",
      "The goal is to aggregate a wide range of Deep Learning topics in one place with a common abstraction, making it easier for you to dive into this fascinating field. Each repository not only focuses on a specific area of Deep Learning, but also includes links to tutorials and reference papers. These resources are carefully selected to help you grasp both the practical and theoretical aspects of Deep Learning, which can sometimes be challenging to find.\n",
      "\n",
      "Text from file `../speech_to_text/README.md`\n",
      "Score : 0.391\n",
      ":yum:\n",
      "\n",
      " ## Project structure\n",
      "\n",
      " ├── architectures            : utilities for model architectures\n",
      "│   ├── layers               : custom layer implementations\n",
      "│   ├── transformers         : transformer architecture implementations\n",
      "│   │   └── whisper_arch.py     : Whisper architecture\n",
      "│   ├── generation_utils.py  : utilities for text and sequence generation\n",
      "│   ├── hparams.py           : hyperparameter management\n",
      "│   └── simple_models.py     : defines classical models such as CNN / RNN / MLP and siamese\n",
      "├── custom_train_objects     : custom objects used in training / testing\n",
      "├── loggers                  : logging utilities for tracking experiment progress\n",
      "├── models                   : main directory for model classes\n",
      "│   ├── interfaces           : directories for interface classes\n",
      "│   ├── stt                  : STT implementations\n",
      "│   │   ├── base_stt.py      : abstract base class for all STT models\n",
      "│   │   └── whisper.py       :\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the meaning of life ?'\n",
    "\n",
    "res = model.retrieve(query, vectors, k = 3)[0]\n",
    "\n",
    "for paragraph in res:\n",
    "    print('Text from file `{filename}`\\nScore : {score:.3f}\\n{text}\\n'.format(** paragraph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e5fa3-3a92-42ab-bb5d-6a90de5250b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

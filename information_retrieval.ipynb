{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eddea025-fb05-4d45-8fde-5060a8b6b388",
   "metadata": {},
   "source": [
    "# Information retrieval\n",
    "\n",
    "Information Retrieval (IR) is a Natural Language Processing (NLP) task, in which the objective is to retrieve relevant information within a corpus of text, based on a query. For this purpose, text encoder models are trained to represent texts using `embeddings` (i.e., vectors of numbers), aiming to correctly represent the meaning of the text. Finally, the model can also encode the query, and hopefully, the query embedding will be closer to text embeddings that are meaningful to answer the query. \n",
    "\n",
    "This notebooks show how to perform Information Retrieval to retrieve relevant spans of texts within a set of documents. This will be divided into 3 different sections : \n",
    "1. **Document processing** : the 1st step is to extract spans of texts from all the desired documents.\n",
    "2. **Text encoding** : the 2nd step is to encode all the spans of texts using an appropriate embedding model. For this demonstration, we will use the [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) model,\n",
    "3. **Search query** : finally, the last step is to encode the query by using the same model, and retrieve the spans with the lowest distance (or higher similarity) with the query embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf8ec35-3787-4ecc-9cfd-ec1eb8436af5",
   "metadata": {},
   "source": [
    "## 1. Document processing\n",
    "\n",
    "The document processing step aims to extract texts from documents. The `parse_document` method accepts filenames, directories and filename format (as below), and returns a list of paragraphs. This is more convenient compared to full text extraction for futher processing, like encoding the texts ;) The method currently accepts `.txt`, `.md`, `.pdf` and `.docs` file formats, and more will be added in the future !\n",
    "\n",
    "For this demonstration, I will use the `README` files from all my github repositories. This will also be easier to evaluate the relevance of the retrieved documents ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c852972-e22b-43bf-ad3f-34385d35334a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# texts : 291\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>section</th>\n",
       "      <th>section_titles</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># :yum: Data processing utilities</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Check the CHANGELOG file to have a global over...</td>\n",
       "      <td>text</td>\n",
       "      <td>1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>## Project structure</td>\n",
       "      <td>text</td>\n",
       "      <td>1.1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Check the provided notebooks to have an overvi...</td>\n",
       "      <td>text</td>\n",
       "      <td>1.1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>├── example_data        : data used for the de...</td>\n",
       "      <td>code</td>\n",
       "      <td>1.1</td>\n",
       "      <td>[:yum: Data processing utilities, Project stru...</td>\n",
       "      <td>../data_processing/README.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  type section  \\\n",
       "0                  # :yum: Data processing utilities  text       1   \n",
       "1  Check the CHANGELOG file to have a global over...  text       1   \n",
       "2                               ## Project structure  text     1.1   \n",
       "3  Check the provided notebooks to have an overvi...  text     1.1   \n",
       "4  ├── example_data        : data used for the de...  code     1.1   \n",
       "\n",
       "                                      section_titles  \\\n",
       "0  [:yum: Data processing utilities, Project stru...   \n",
       "1  [:yum: Data processing utilities, Project stru...   \n",
       "2  [:yum: Data processing utilities, Project stru...   \n",
       "3  [:yum: Data processing utilities, Project stru...   \n",
       "4  [:yum: Data processing utilities, Project stru...   \n",
       "\n",
       "                       filename  \n",
       "0  ../data_processing/README.md  \n",
       "1  ../data_processing/README.md  \n",
       "2  ../data_processing/README.md  \n",
       "3  ../data_processing/README.md  \n",
       "4  ../data_processing/README.md  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from utils.text import parse_document\n",
    "\n",
    "documents = parse_document('../**/README.md')\n",
    "documents = pd.DataFrame(documents)\n",
    "print('# texts : {}'.format(len(documents)))\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5935181-3802-4327-accc-17c0e7ef0417",
   "metadata": {},
   "source": [
    "## 2. Text encoding\n",
    "\n",
    "Now that we have all the texts extracted with additional information (like section title / filename), we can encode them using embeddings ! For this purpose, let's initialize a `TextEncoder` model with the [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3) model. Then, we can use this model to encode the texts by using the `embed` method. \n",
    "\n",
    "`embed` is a batched function, meaning that you can provide the `batch_size` argument to control the number of texts to encode in parallel. An important aspect to consider is that texts are padded when passing in parallel, in order to form a rectangular matrix (i.e., the smaller texts have zero-values at the end so that all texts within a batch have the same length). The function has been optimized by sorting the texts by length, in order to minimize padding. However, it remains interesting to correctly tune the `batch_size`, as it has a large impact on performances ! My recommandation would be to use a small value, around 8. \n",
    "\n",
    "The model is compiled using `XLA` by default, which explains why some calls are slower than subsequent ones, due to retracing. \n",
    "\n",
    "At the 1st call, the official `transformers` model will be downloaded and converted to my `keras` implementation of the `XLMRoberta` architecture. For this purpose, you will need the `torch` library to be installed. Once done, the model will be saved in regular keras format under the `pretrained_models/{name}` folder for subsequent loading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b314a0b8-7b17-4a88-99b2-ba44ae7a8e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== bge-m3 ==========\n",
      "Model instance `model`\n",
      "- Inputs \t: unknown\n",
      "- Outputs \t: unknown\n",
      "- Number of layers \t: 26\n",
      "- Number of parameters \t: 567.756 Millions\n",
      "- Model not compiled yet\n",
      "\n",
      "Transfer-learning from : BAAI/bge-m3\n",
      "Already trained on 0 epochs (0 steps)\n",
      "\n",
      "- Embedding dim   : 1024\n",
      "- Distance metric : cosine\n",
      "- Language : multi\n",
      "- Vocabulary (size = 250002) : ['<s>', '<pad>', '</s>', '<unk>', ',', '.', '▁', 's', '▁de', '-', '▁a', 'a', ':', 'e', 'i', '▁(', ')', '▁i', 't', 'n', '▁-', '▁la', '▁en', '▁in', '▁na', ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from models.encoder.text_encoder import TextEncoder\n",
    "\n",
    "model = TextEncoder(pretrained = 'BAAI/bge-m3', name = 'bge-m3')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f5e2c91-26c8-43da-9f8c-4d3bc3b288b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 37/37 [00:03<00:00, 11.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DenseVectors ==========\n",
      "- # data    : 291\n",
      "- Dimension    : 1024\n",
      "- Data keys    : ('text', 'type', 'section', 'section_titles', 'filename')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "vectors = model.embed(documents, batch_size = 8, tqdm = tqdm)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc93ccc-1eee-4e36-a9a9-6be7a06eb2aa",
   "metadata": {},
   "source": [
    "## 3. Search query\n",
    "\n",
    "The final step is to encode the query, then compute the `cosine similarity` (or any other distance/similarity metric) between the embedded query and all embedded texts, and take the top-k with the best score ! All these steps are performed internally by the `search` method of the `DenseVectors` class ;)\n",
    "\n",
    "We can observe that the best results are correctly related to `embeddings`, and even more, the best retrieved passage correctly defines the notion of embeddings !\n",
    "\n",
    "It is worth mentioning that the model will retrieve passages no matter if they are relevant or not, as it simply provides a score for each passage. Therefore, if the query does not have any relevant span in the provided text, it will return irrelevant spans. Nonetheless, as it can be observed in the 2nd example, scores for such irreevant passages is *significantly lower* (cosine similarity is between 0 and 1) than relevant ones (in the 1st example) ;) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbb7c8-ef9a-40bb-9969-fd8dc1e25761",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c10158c9-7469-4636-a1b1-66c6be814a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DenseVectors ==========\n",
      "- # data    : 3\n",
      "- Dimension    : 1024\n",
      "- Data keys    : ('text', 'type', 'section', 'section_titles', 'filename', 'score', 'index')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'What is an embedding ?'\n",
    "\n",
    "res = vectors.search(query, k = 3)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb2760e1-94f9-4150-947c-b47a7461d363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from file `../text_to_speech/README.md` - section [':yum: Text To Speech (TTS)', 'Multi-speaker Text-To-Speech', 'Automatic voice cloning with the `SV2TTS` architecture', 'The basic intuition']\n",
      "Score : 0.517\n",
      "Here is a short overview of the proposed procedure :\n",
      "1. Train a model to identify speakers based on short audio samples : the `speaker verification` model. This model basically takes as input an audio sample (5-10 sec) from a speaker, and encodes it on a *d*-dimensional vector, named the `embedding`. This embedding aims to capture relevant information about the speaker's voice (e.g., `frequencies`, `rythm`, `pitch`, ...). \n",
      "2. This pre-trained `Speaker Encoder (SE)` is then used to encode the voice of the speaker to clone.\n",
      "3. The produced embedding is then concatenated with the output of the `Tacotron-2` encoder part, such that the `Decoder` has access to both the encoded text and the speaker embedding.\n",
      "\n",
      "Text from file `../data_processing/README.md` - section [':yum: Data processing utilities', 'Notes and references']\n",
      "Score : 0.517\n",
      "- The provided embeddings in `example_data/embeddings/embeddings_256_voxforge.csv` has been generated based on samples of the VoxForge dataset, and embedded with an AudioSiamese model (`audio_siamese_256_mel_lstm`).\n",
      "\n",
      "Text from file `../text_to_speech/README.md` - section [':yum: Text To Speech (TTS)', 'Multi-speaker Text-To-Speech', 'Automatic voice cloning with the `SV2TTS` architecture', 'The basic intuition']\n",
      "Score : 0.507\n",
      "The objective is that the `Decoder` will learn to use the `speaker embedding` to copy its prosody / intonation / ... to read the text with the voice of this speaker.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paragraph in res:\n",
    "    print('Text from file `{filename}` - section {section_titles}\\nScore : {score:.3f}\\n{text}\\n'.format(** paragraph))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbde3d0-0832-4a46-a6cd-740f50d0e116",
   "metadata": {},
   "source": [
    "### Example 2 : irrelevant query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc3b922-3386-4a6b-bf79-d22da07b6faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== DenseVectors ==========\n",
      "- # data    : 3\n",
      "- Dimension    : 1024\n",
      "- Data keys    : ('text', 'type', 'section', 'section_titles', 'filename', 'score', 'index')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'What is the meaning of life ?'\n",
    "\n",
    "res = vectors.search(query, k = 3)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6a7a488-a53f-42ba-adc3-9a65c02395e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text from file `../yui-mhcp/README.md` - section nan\n",
      "Score : 0.430\n",
      "Welcome to my GitHub profile ! The objective of my projects is to make Deep Learning more accessible, and provide real-world examples to enhance education and research in the field ! :yum:\n",
      "\n",
      "Text from file `../detection/README.md` - section [':yum: Object detection', 'Available features']\n",
      "Score : 0.417\n",
      "| Feature   | Fuction / class   | Description |\n",
      "| :-------- | :---------------- | :---------- |\n",
      "| detection | `detect`  | detect objects on images / videos and allow multiple saving types (save cropped boxes, detected images, video frames, ...)    |\n",
      "| stream    | `stream`  | perform detection on your camera (also allow to save frames) |\n",
      "\n",
      "Text from file `../text_to_speech/README.md` - section [':yum: Text To Speech (TTS)', 'Multi-speaker Text-To-Speech', 'Automatic voice cloning with the `SV2TTS` architecture', 'The basic intuition']\n",
      "Score : 0.388\n",
      "#### The basic intuition\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paragraph in res:\n",
    "    print('Text from file `{filename}` - section {section_titles}\\nScore : {score:.3f}\\n{text}\\n'.format(** paragraph))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026e5fa3-3a92-42ab-bb5d-6a90de5250b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
